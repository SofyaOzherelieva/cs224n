{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78SbbUlacgDO"
   },
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfouRZt4cgDQ"
   },
   "source": [
    "## Word2vec preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQS99SxZcgDa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PlWVkILccgDe",
    "outputId": "8a031c08-bd15-4889-8ccb-c18fc81846dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = False\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEdSZOus8E33"
   },
   "outputs": [],
   "source": [
    "class CBOWBatcher:\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.vocab_size = len(words)\n",
    "        self.remaining_words = self.words\n",
    "    \n",
    "        self.index_words = []\n",
    "        self.word2index = dict()\n",
    "        self.index2word = dict()\n",
    "        self.window_size = 0\n",
    "        \n",
    "    def delete_rare_words(self, min_frequency = 9):\n",
    "        words_count = Counter(self.words)\n",
    "        \n",
    "        sorted_words_count = sorted(words_count.items(), key=lambda pair: pair[1], reverse=True)\n",
    "\n",
    "        # choose words with frequency > min_frequency\n",
    "        mask = list(map(lambda x: x[1] > min_frequency, sorted_words_count))\n",
    "        sorted_words_count = np.array(sorted_words_count)[mask]\n",
    "\n",
    "        # add __unk__\n",
    "        remaining_words = list(map(lambda x: x[0], sorted_words_count))\n",
    "        remaining_words.append(\"__unk__\")\n",
    "        self.remaining_words = remaining_words\n",
    "        self.vocab_size = len(remaining_words)\n",
    "    \n",
    "    def create_indexing(self):\n",
    "        # numericalization\n",
    "        numbers = np.arange(len(self.remaining_words))\n",
    "\n",
    "        self.word2index = dict(zip(self.remaining_words, numbers))\n",
    "        self.index2word = dict(zip(numbers, self.remaining_words))\n",
    "        # __unk__\n",
    "        for word in self.words:\n",
    "            if word in self.word2index:\n",
    "                self.index_words.append(self.word2index[word])\n",
    "            else:\n",
    "                self.index_words.append(self.word2index[\"__unk__\"])\n",
    "        \n",
    "    def CBOW(self, window_size = 2):\n",
    "        self.window_size = window_size\n",
    "        x_batch = []\n",
    "        labels_batch = []\n",
    "        \n",
    "        for i in np.arange(window_size, len(self.index_words) - window_size):\n",
    "            labels_batch.append(self.index_words[i])\n",
    "            temp_list = self.index_words[i - window_size: i] + self.index_words[i+1: i+window_size+1]\n",
    "            x_batch.append(temp_list)\n",
    "        return x_batch, labels_batch\n",
    "    \n",
    "    def indices_to_words(self, batch):\n",
    "        words_batch = []\n",
    "        shape = np.array(batch).shape\n",
    "        batch_flatten = np.array(batch).flatten()\n",
    "        \n",
    "        for i in batch_flatten:\n",
    "            if i in self.index2word:\n",
    "                words_batch.append(self.index2word[i])\n",
    "            else:\n",
    "                raise Exception(\"Incorrect numericalization: {} in {}\".format(i, batch))\n",
    "                \n",
    "        words_batch = np.array(words_batch).reshape(shape)\n",
    "        return words_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "MwAzYwEF8UcW",
    "outputId": "5a03a667-a7d7-42c5-f5ac-170894f5c818"
   },
   "outputs": [],
   "source": [
    "text_file = open(\"text8\", \"r\")\n",
    "words = text_file.read().split(' ')\n",
    "test = words[:10]\n",
    "\n",
    "batcher = CBOWBatcher(test)\n",
    "batcher.delete_rare_words(0)\n",
    "batcher.create_indexing()\n",
    "y = batcher.CBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot_generate(batches, labels, vocab_size):\n",
    "    for batch, label in zip(batches, labels):\n",
    "        batch = np.array(batch)\n",
    "        oneHot_batch = np.zeros((batch.shape[0], vocab_size))\n",
    "        oneHot_batch[np.arange(batch.shape[0]), batch] = 1\n",
    "\n",
    "        oneHot_label = np.zeros((vocab_size))\n",
    "        oneHot_label[label] = 1\n",
    "        yield (oneHot_batch, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VShZDVQzcgDk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5414,  0.0847, -0.0120,  0.5146, -0.4327], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        in_ = context.sum(dim = 0, dtype = torch.float32)\n",
    "        out = self.layer1(in_)\n",
    "        out = out/context.shape[0]\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test_CBOW():\n",
    "    vocab_size = 5\n",
    "    embedding_dim = 2\n",
    "    context_size = 2\n",
    "    hidden_size = 4\n",
    "    x = torch.tensor([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0]], dtype = torch.float32)\n",
    "    x = x.to(device=device, dtype=torch.float32)\n",
    "    model = CBOW(vocab_size, hidden_size)\n",
    "    scores = model(x)\n",
    "    print(scores)\n",
    "    \n",
    "test_CBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UhZ7fXwShp_k"
   },
   "outputs": [],
   "source": [
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qnmlYlgcgDo"
   },
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, contexts, labels, epochs=1):\n",
    "    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(contexts, labels)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for t, (context, label) in enumerate(oneHot_generate(X_train, y_train)):\n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            x = torch.tensor([w for w in context], dtype=torch.float32)        \n",
    "#             y = torch.tensor([label], dtype=torch.float32)\n",
    "            \n",
    "            x = x.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            #print(scores, y, scores.shape, y.shape)\n",
    "            loss = loss_func(scores, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-78bf392895c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "hidden_size = 4\n",
    "model = CBOW(batcher.vocab_size, hidden_size)\n",
    "optimizer = optim.SGD(model.parameters)\n",
    "train_part34(model, optimizer, y[0], y[1], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJYQLooNhIQd"
   },
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)        \n",
    "            y = torch.tensor([word_to_ix[y]], dtype=torch.long)\n",
    "\n",
    "\n",
    "            x = x.to(device=device, dtype=torch.long)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y)\n",
    "            num_samples += 1\n",
    "        print(num_correct, num_samples)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got {} acc'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVk6-F3ycgD7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task3_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
