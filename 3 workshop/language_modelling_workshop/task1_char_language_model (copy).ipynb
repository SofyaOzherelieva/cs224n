{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посимвольная языковая модель.\n",
    "\n",
    "В первом задании Вам нужно написать и обучить посимвольную нейронную языковую модель для вычисления вероятностей буквенных последовательностей (то есть слов). Такие модели используются в задачах словоизменения и распознавания/порождения звучащей речи. Для обучения модели используйте данные для русского языка из [репозитория](https://github.com/sigmorphon/conll2018/tree/master/task1/surprise).\n",
    "\n",
    "**В процессе написания Вам нужно решить следующие проблемы:**\n",
    "    \n",
    "* как будет выглядеть обучающая выборка; что будет являться признаками, и что - метками классов.\n",
    "* как сделать так, чтобы модель при предсказании символа учитывала все предыдущие символы слова.\n",
    "* какие специальные символы нужно использовать.\n",
    "* как передавать в модель текущее состояние рекуррентной сети\n",
    "\n",
    "**Результаты:**\n",
    "\n",
    "* предобработчик данных,\n",
    "* генератор обучающих данных (батчей),\n",
    "* обученная модель\n",
    "* перплексия модели на настроечной выборке\n",
    "* посимвольные вероятности слов в контрольной выборке\n",
    "\n",
    "**Дополнительно:**\n",
    "\n",
    "* дополнительный вход модели (часть речи слова, другие морфологические признаки), влияет ли его добавление на перплексию\n",
    "* сравнение различных архитектур нейронной сети (FC, RNN, LSTM, QRNN, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобработка данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Uncomment to download data\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-train-high\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-dev\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-covered-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переписала функцию считывания данных, потому что прошлая мне не нравилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(infile):\n",
    "    df = pd.read_html(infile)\n",
    "    df = df[0]\n",
    "    df.columns = [\"x\", \"y\"]\n",
    "    df[\"y\"] = df[\"y\"].astype('str')\n",
    "    df = df.drop(columns=[\"x\"])\n",
    "    x = df.y.str.split(\"\\t\",expand=True,)\n",
    "    if x.shape[1] == 3:\n",
    "        df['word'] = x[0].str.lower()\n",
    "        df['tag'] = x[2]\n",
    "    else:\n",
    "        df['word'] = x[0].str.lower()\n",
    "        df['tag'] = x[1]\n",
    "    df = df.drop(columns = [\"y\"])\n",
    "    \n",
    "    words = list(df['word'])\n",
    "    tags = list(df['tag'])\n",
    "    return words, tags\n",
    "\n",
    "train_words, train_tags = read_dataset(\"russian-train-high\")\n",
    "dev_words, dev_tags = read_dataset(\"russian-dev\")\n",
    "test_words, test_tags = read_dataset(\"russian-covered-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подумайте, какие вспомогательные токены могут быть вам полезны. Выдайте им индексы от `0` до `len(AUXILIARY) - 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится символ конца слова!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUXILIARY = ['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    symbols = 0\n",
    "    symbol_codes= dict()\n",
    "    def fit(self, data):\n",
    "        \"\"\"Extract unique symbols from the data, make itos (item to string) and stoi (string to index) objects\"\"\"\n",
    "        symbols = set(x for elem in data for x in elem)\n",
    "        self.symbols = AUXILIARY + sorted(symbols)\n",
    "        # Запомните следующую строчку кода - она нужна примерно всегда\n",
    "        self.symbol_codes = {s: i for i, s in enumerate(self.symbols)}\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._symbols)\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Transform data to indices\n",
    "        Input:\n",
    "            - data, list of strings\n",
    "        Output:\n",
    "            - list of list of char indices\n",
    "\n",
    "        >>> self.transform(['word1', 'token2'])\n",
    "        >>> [[24, 2, 19, 13, 3], [8, 2, 9, 1, 7, 4]]\n",
    "        \"\"\"\n",
    "        max_len = max(map(len, train_words))\n",
    "        return [[self.symbol_codes[char] for char in word] + \n",
    "                [self.symbol_codes[AUXILIARY[0]]]*(max_len - len(word) + 1) for word in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabulary at 0x7ff9e9c99eb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = Vocabulary()\n",
    "v.fit(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforned_train_words = v.transform(train_words)\n",
    "transforned_dev_words = v.transform(dev_words)\n",
    "transforned_test_words = v.transform(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(map(len, transforned_train_words))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генератор обучающих данных (батчей):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(data, batch_size = 4):\n",
    "    data = np.array(data)\n",
    "    data = data.reshape(int(data.shape[0]/batch_size), batch_size, data.shape[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(len(v.symbols), input_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.embedding(input)\n",
    "        input_combined = torch.cat((emb, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def train(input_line_tensor):\n",
    "    \n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    n = len(input_line_tensor)\n",
    "    for i in range(n - 1):\n",
    "        input = torch.Tensor([input_line_tensor[i]]).long()\n",
    "        target = torch.Tensor([input_line_tensor[i+1]]).long()\n",
    "        \n",
    "        output, hidden = rnn(input, hidden)\n",
    "        l = criterion(output, target)\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0 0%) 3.4145\n",
      "(250 2%) 0.7661\n",
      "(500 5%) 0.7460\n",
      "(750 7%) 0.6044\n",
      "(1000 10%) 0.4997\n",
      "(1250 12%) 0.6099\n",
      "(1500 15%) 0.4826\n",
      "(1750 17%) 0.8114\n",
      "(2000 20%) 0.4596\n",
      "(2250 22%) 0.7487\n",
      "(2500 25%) 0.5445\n",
      "(2750 27%) 0.7518\n",
      "(3000 30%) 0.6156\n",
      "(3250 32%) 0.4169\n",
      "(3500 35%) 0.6279\n",
      "(3750 37%) 0.3791\n",
      "(4000 40%) 0.6267\n",
      "(4250 42%) 0.6295\n",
      "(4500 45%) 0.4878\n",
      "(4750 47%) 0.5989\n",
      "(5000 50%) 0.8679\n",
      "(5250 52%) 0.6289\n",
      "(5500 55%) 0.3339\n",
      "(5750 57%) 0.8290\n",
      "(6000 60%) 0.5071\n",
      "(6250 62%) 0.5527\n",
      "(6500 65%) 0.6039\n",
      "(6750 67%) 0.7091\n",
      "(7000 70%) 0.5788\n",
      "(7250 72%) 0.6447\n",
      "(7500 75%) 0.5301\n",
      "(7750 77%) 0.9129\n",
      "(8000 80%) 0.5520\n",
      "(8250 82%) 0.4272\n",
      "(8500 85%) 0.7750\n",
      "(8750 87%) 0.5702\n",
      "(9000 90%) 0.4974\n",
      "(9250 92%) 0.6530\n",
      "(9500 95%) 0.5785\n",
      "(9750 97%) 0.6883\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "\n",
    "b_train = transforned_train_words# batcher(transforned_train_words, batch_size)\n",
    "b_dev = transforned_dev_words #batcher(transforned_dev_words, batch_size)\n",
    "b_test = transforned_test_words #batcher(transforned_test_words, batch_size)\n",
    "\n",
    "rnn = RNN(10, 128, len(v.symbols))\n",
    "\n",
    "n_batches = len(b_train)\n",
    "print_every = n_batches/40\n",
    "plot_every = n_batches/40\n",
    "all_losses = []\n",
    "\n",
    "for i in range(n_batches):\n",
    "    output, loss = train(b_train[i])\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print('(%d %d%%) %.4f' % (i, i / n_batches * 100, loss))\n",
    "\n",
    "    if i % plot_every == 0:\n",
    "        all_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-589260463a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перплексия модели на настроечной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(input):\n",
    "    with torch.no_grad():\n",
    "        prob = []\n",
    "        loss= 0\n",
    "        predicted = [v.symbols[input[0]]]\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        for i in range(max_len-1):\n",
    "            x =  torch.Tensor([input[i]]).long()\n",
    "            output, hidden = rnn(x, hidden)\n",
    "            target = input[i+1]\n",
    "            l = criterion(output, torch.Tensor([target]).long())\n",
    "            \n",
    "            output = np.exp(list(output[0]))\n",
    "            \n",
    "            prob.append((v.symbols[target], output[target]))\n",
    "            \n",
    "            pred = np.argmax(output)\n",
    "            letter = v.symbols[pred]\n",
    "            if letter == '\\n':\n",
    "                if v.symbols[target] == '\\n':\n",
    "                    break\n",
    "            else:\n",
    "                predicted.append(letter)\n",
    "                loss += l\n",
    "\n",
    "        return predicted, prob, loss/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, i in enumerate(b_dev):\n",
    "    total_loss = test(i)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перплексия модели на настроечной выборке: 1.0013777017593384\n"
     ]
    }
   ],
   "source": [
    "print(\"Перплексия модели на настроечной выборке:\", float(2**(total_loss/len(b_dev))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посимвольные вероятности слов в контрольной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мальтийский\n",
      "[('а', 0.25050285), ('л', 0.054715242), ('ь', 0.1101205), ('т', 0.009931438), ('и', 0.086654566), ('й', 0.039271723), ('с', 0.009113473), ('к', 0.08444033), ('и', 0.20869167), ('й', 0.7848847), ('\\n', 0.09413854), ('\\n', 0.99466133)]\n",
      "расчленить\n",
      "[('а', 0.3061611), ('с', 0.10751787), ('ч', 0.0077340878), ('л', 0.0313337), ('е', 0.11856763), ('н', 0.21130224), ('и', 0.028132921), ('т', 0.18241128), ('ь', 0.8329007), ('\\n', 0.5418521)]\n",
      "лопаться\n",
      "[('о', 0.2504094), ('п', 0.049267158), ('а', 0.10099998), ('т', 0.27762204), ('ь', 0.5657092), ('с', 0.17100863), ('я', 0.7871049), ('\\n', 0.6988299)]\n",
      "индексировать\n",
      "[('н', 0.029501518), ('д', 0.019726587), ('е', 0.14537437), ('к', 0.03980444), ('с', 0.14421499), ('и', 0.080768764), ('р', 0.050032053), ('о', 0.09853223), ('в', 0.27753305), ('а', 0.19649972), ('т', 0.6828432), ('ь', 0.05573856), ('\\n', 0.62151325)]\n",
      "своевременный\n",
      "[('в', 0.009151097), ('о', 0.14734623), ('е', 0.040518988), ('в', 0.016498104), ('р', 0.12405084), ('е', 0.12884106), ('м', 0.03500889), ('е', 0.049088918), ('н', 0.25541228), ('н', 0.24604918), ('ы', 0.660237), ('й', 0.96873146), ('\\n', 0.97271824)]\n",
      "расправить\n",
      "[('а', 0.3825288), ('с', 0.0170222), ('п', 0.066002876), ('р', 0.25041628), ('а', 0.20005012), ('в', 0.026748093), ('и', 0.1557398), ('т', 0.4694094), ('ь', 0.8433309), ('\\n', 0.7398746)]\n",
      "заторопиться\n",
      "[('а', 0.15768708), ('т', 0.13390434), ('о', 0.124343574), ('р', 0.087529905), ('о', 0.09135692), ('п', 0.022084326), ('и', 0.18088794), ('т', 0.2663943), ('ь', 0.7161167), ('с', 0.164891), ('я', 0.8056592), ('\\n', 0.7335598)]\n",
      "рассвет\n",
      "[('а', 0.30923018), ('с', 0.09959693), ('с', 0.025635252), ('в', 0.042367212), ('е', 0.084799804), ('т', 0.3307672), ('\\n', 0.0061095725), ('\\n', 0.98720604)]\n",
      "красить\n",
      "[('р', 0.09748202), ('а', 0.20022991), ('с', 0.023919465), ('и', 0.060647395), ('т', 0.2098457), ('ь', 0.47335854), ('\\n', 0.5312143)]\n",
      "переустроить\n",
      "[('е', 0.10499047), ('р', 0.30688885), ('е', 0.16113417), ('у', 0.016376514), ('с', 0.021128742), ('т', 0.33571982), ('р', 0.05920017), ('о', 0.10833913), ('и', 0.016801309), ('т', 0.37518495), ('ь', 0.64887196), ('\\n', 0.5704523)]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in b_test[:10]:\n",
    "    res = test(batch)[1]\n",
    "    print(test_words[i])\n",
    "    i += 1\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По вероятностям видно, что модель хорошо угадывает окончания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
