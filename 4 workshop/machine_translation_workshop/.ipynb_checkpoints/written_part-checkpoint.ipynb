{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Machine Translation with RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* g) \n",
    "    * при вычислении e_t с помощью enc_masks мы выставляем -inf там где у enc_masks стоят 1, то есть там где в предложении стоит \"pad\" токен\n",
    "    * attention нужен для того, чтобы нейросеть обратила внимание на конкретные слова в предложении (не на \"pad\"). При вычислении распределения на словах с помощью softmax(e_t) мы не будем таким образом обращать внимание на \"pad\" так как $e^{-\\infty} = 0$\n",
    "\n",
    "\n",
    "* i) BLEU Score: 22.626626524294\n",
    "\n",
    "\n",
    "* j) \n",
    "    * Dot product attention:\n",
    "        * Плюсы: самая простая модель, нет параметров\n",
    "        * Минусы: из-за отсутствия параметров сеть не может обучиться выкидывать то, что ей не нужно\n",
    "    * Multiplicative attention: (используется в коде)\n",
    "        * Плюсы: обучается учитывать или не учитывать скрытые состояния энкодера\n",
    "        * Минусы: есть склонность к переобучению\n",
    "    * Additive attention:\n",
    "        * Плюсы: обучается как на скрытых состояниях энкодера, так и на скрытом состоянии декодера. То есть сеть таким образом может уловить больше зависимостей.\n",
    "        * Минусы: примерно в 2 раза больше параметров, есть склонность к переобучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing NMT Systems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a)\n",
    "    * i) \n",
    "        * Ошибка: favorite of my favorites - не имеет смысла\n",
    "        * Причина: возможно модель не видела такой языковой конструкции в обучающей выборке, поэтому не смогла сгенерировать one of my favorites\n",
    "        * Возможное решение: добавить таких примеров в обучающую выборку\n",
    "    * ii) \n",
    "        * Ошибка: I’m probably the author for children \n",
    "        * Причина: дословный перевод el autor para ni˜nos\n",
    "        * Возможное решение: возможно стоит попробовать другой attention, чтобы оно лучше обращала внимание на языковые конструкции в целом\n",
    "    * iii) \n",
    "        * Ошибка: unk токен\n",
    "        * Причина: слово не попадалось в обучающей выборке\n",
    "        * Возможное решение: расширить словарь, переводить незнакомые слова побуквенно или по кусочкам, то есть использовать его символьное представление\n",
    "    * iv) \n",
    "        * Ошибка: manzana == apple, однако в данном контексте manzana == block\n",
    "        * Причина: смещение датасета в сторону manzana == apple. То есть было много примеров manzana == apple и мало manzana == block\n",
    "        * Возможное решение: добавить больше примеров manzana == block в датасет или можно попробовать другой тип attention с меньшей вероятностью переобучиться (dot product attention) или с возможностью уловить больше зависимостей (additive attention). Нужно экспериментировать!\n",
    "    * v) \n",
    "        * Ошибка: women’s room != teachers’ lounge\n",
    "        * Причина: обратив внимание на слово \"she\" нейросеть выдала \"woman\" вместо \"teacher\". То есть в обучающей выборке было смещение.\n",
    "        * Возможное решение: добавить больше примеров \"profesores\" == \"teachers\"    \n",
    "    * vi) \n",
    "        * Ошибка: неправильно переведенные единицы измерения hect´areas != acres\n",
    "        * Причина: встречаемость слов hect´areas, acres в одном контексте\n",
    "        * Возможное решение: захардкодить перевод единиц измерения?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b)\n",
    "    * i) \n",
    "        * Source sentence in Spanish.: Tena un pulgar, y 85 dlares, y termin en San Francisco, California -- encontr un amante -- y en los aos '80, sent la necesidad de comenzar a trabajar en organizaciones que luchaban contra el SIDA.\n",
    "        * Reference English translation: I had a thumb, I had 85 dollars,  and I ended up in San Francisco, California --  met a lover --  and back in the '80s, found it necessary  to begin work on AIDS organizations.\n",
    "        * NMT model’s English translation: I had a thumb, and 85 dollars, and I ended up in San Francisco, California -- I found a lover -- and in the 1920s, I felt the need to start working in organizations fighting against AIDS.\n",
    "        * Ошибка: and in the 1920s != in the '80s\n",
    "        * Причина: возможно в обучающем датасете попалось число 1920 примерно в таком же контексте\n",
    "        * Возможное решение: добавить больше примеров с числами\n",
    "    * ii) \n",
    "        * Source sentence in Spanish.: Estoy desilusionada que de adultos nunca llegamos a conocernos.\n",
    "        * Reference English translation: I'm disappointed  that we never got to know each other as adults.\n",
    "        * NMT model’s English translation: I'm \"unk\" that of adults never come to know us.\n",
    "        * Ошибка: неправильно переведенная инверсия que de adultos nunca llegamos a conocernos\n",
    "        * Причина: такая инверсия не встретилась в обучающей выборке\n",
    "        * Возможное решение: расширить обучающую выборку подобными конструкциями       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* с)\n",
    "    * i)\n",
    "        * c1: \n",
    "            * p1 = 0.6\n",
    "            * p2 = 0.5\n",
    "            * c = 5\n",
    "            * $r^*$ = 4\n",
    "            * BP = 1\n",
    "            * BLEU = 0.547723\n",
    "        * c2: \n",
    "            * p1 = 0.8\n",
    "            * p2 = 0.5\n",
    "            * c = 5\n",
    "            * $r^*$ = 4\n",
    "            * BP = 1\n",
    "            * BLEU = 0.632456\n",
    "        \n",
    "       То есть метрика BLEU показала, что c2 ближе к правильному переводу. Я согласна, с2 правда лучше передаёт смысл исходного предложения\n",
    "    * ii)\n",
    "        * c1: \n",
    "            * p1 = 0.6\n",
    "            * p2 = 0.5\n",
    "            * c = 5\n",
    "            * $r^*$ = 6\n",
    "            * BP = $e^{-1/5}$\n",
    "            * BLEU = 0.448437\n",
    "        * c2: \n",
    "            * p1 = 0.4\n",
    "            * p2 = 0.25\n",
    "            * c = 5\n",
    "            * $r^*$ = 6\n",
    "            * BP = $e^{-1/5}$\n",
    "            * BLEU = 0.258905\n",
    "        \n",
    "        Метрика BLEU теперь показала, что c1 ближе к правильному переводу. С этим сложно согласиться, с2 лучше передаёт смысл исходного предложения.\n",
    "    * iii) \n",
    "    \n",
    "    Мы видим, что это может быть проблемой из примера выше. Если нашей системе доступен только один вариант правильного перевода при обучении она может переобучиться под него. \n",
    "   \n",
    "   Это противоречит идее, что нет единственно верного перевода.\n",
    "    * iv)\n",
    "        * Плюсы: \n",
    "            * удобно считать, хорошая замена оценки качества с помощью привлечения людей\n",
    "            * даёт достаточно справедливую оценку во многих случаях.\n",
    "        * Минусы:\n",
    "            * хороший перевод может иметь низкий BLEU, так как BLEU смотрит на n-граммы. То есть минусом является сама идея BLEU: мы не смотрим на смысл целиком, а подгоняем под единственный пример, который считаем верным. (но тут наверное нет лучшей альтернативы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
